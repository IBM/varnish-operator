# VarnishCluster

### Creating a `VarnishCluster` Resource

Once you've [installed the operator](installation.md), you can create a new type of resource - `VarnishCluster`.

As with any other Kubernetes resource, simply `kubectl apply -f <varnish-cluster>.yaml` to create a `VarnishCluster`.

Here's an example of a simple `VarnishCluster`:

```yaml
apiVersion: icm.ibm.com/v1alpha1
kind: VarnishCluster
metadata:
  labels:
    operator: varnish
  name: varnish-cluster-example
spec:
  vcl:
    configMapName: vcl-config
    entrypointFileName: entrypoint.vcl
  varnish:
    imagePullSecret: docker-reg-secret
  backend:
    selector:
      app: nginx
  service:
    port: 8080
``` 

Make sure you've [created the image pull secret](https://pages.github.ibm.com/TheWeatherCompany/icm-docs/managed-kubernetes/container-registry.html#pulling-an-image-in-kubernetes) required to pull images from the private container registry.
 
Once the `VarnishCluster` is created, you should see:

* a StatefulSet with the name `<varnish-cluster-name>`.
* 2 services, one `<varnish-cluster-name>` and one `<varnish-cluster-name>-no-cache`. As is implied by the names, using `<varnish-cluster-name>` will target the Varnish pods, while `<varnish-cluster-name>-no-cache` will target the underlying pods directly, with no Varnish caching.
* A ConfigMap with VCL in it (either user-created, before running `kubectl apply -f <varnish-cluster>.yaml`, or generated by operator)
* A role/rolebinding/clusterrole/clusterrolebinding/serviceAccount combination to give the Varnish pods the ability to access necessary resources.
* If configured, a PodDisruptionBudget

See [VarnishCluster configuration section](varnish-cluster-configuration.md) for all available fields for configuration.

### VarnishCluster Status

The VarnishCluster keeps track of its current status as events occur in the system. This can be seen through the `Status` field, visible from `kubectl describe vc <your-varnishcluster>`.

### Labels

The labels set for `VarnishCluster` are inherited by all dependent components (Service, StatefulSet, etc.).

Besides custom labels, the operator sets its own:

 * `varnish-owner` - name of the VarnishCluster a particular component belongs to
 * `varnish-component` - name of the component

The `varnish-component` could have the following values:

 * `varnish` - Varnish pod
 * `cache-service` - Kubernetes Service exposing Varnish instances
 * `nocache-service` - Kubernetes Service exposing backends bypassing Varnish
 * `clusterrole` - The clusterrole defining the cluster level permissions for a particular `VarnishCluster`
 * `clusterrolebinding` - Binds the clusterrole to the serviceaccount used by a particular `VarnishCluster`
 * `role` - The role defining the namespaced permissions for a particular `VarnishCluster`
 * `rolebinding` - Binds the role to the serviceaccount used by a particular `VarnishCluster`
 * `vcl-file-configmap` - The ConfigMap that stores the VCL files
* `secret` - The Secret to keep `varnishadm` auth credentials
 * `headless-service` - A headless service that backs the StatefulSet
 * `poddisruptionbudget` - PodDisruptionBudget configuration for a particular `VarnishCluster`
 * `serviceaccount` - A serviceaccount used by the Varnish pods

### Updating a VarnishCluster Resource

Just as with any other Kubernetes resource, using `kubectl apply`, `kubectl patch`, or `kubectl replace` will all update the `VarnishCluster` appropriately. The operator will handle how that update propagates to its dependent resources. Conversely, trying to modify any of those dependent resources (StatefulSet, Services, Roles/Rolebindings, etc) will cause the operator to revert those changes, in the same way a Deployment does for its Pods. The only exception to this is the ConfigMap, the contents of which you [can and should modify](vcl-configuration.md), since that is the VCL used to run the Varnish Pods.

Some spec changes (like the image version or container config change) need a pod restart in order to be applied. As Varnish is an in-memory cache, it means cache data loss. To prevent accidental cache loss, by default, the update strategy is `OnDelete` which means the pods won't automatically get restarted. To update the pod you need to delete the pod manually, and it will come back with the new configuration. This behavior can be changed by setting the desired update strategy in the `.spec.updateStrategy` object. See [VarnishCluster Configuration](varnish-cluster-configuration.md) section for more details.

#### DelayedRollingUpdate

Besides standard update strategies like `OnDelete` and `RollingUpdate`, the Varnish Operator has an another one - `DelayedRollingUpdate`.

`DelayedRollingUpdate` works the same way as `RollingUpdate` with the difference being a pause between pod updates. This is useful when all you want is to wait for a recently updated Varnish node to warm up before an another one reloads. The delay is configurable by the `.spec.updateStrategy.delayedRollingUpdate.delaySeconds` field. 

The delay is counted from the time the pod is created in Kubernetes, not when it got scheduled and become ready so this has to be considered when choosing the delay time.

The operator respects Pods readiness and does not reload the next pod until all pods are ready, even if the delay time elapsed. 

#### Forcefully restarting Varnish pods

Sometimes it is necessary to purge the cluster cache. For example, when a backend with a bug produced a bad response that got cached. After the fix is deployed, we need to purge the cache. This can be achieved by simply restarting the pods. 

If you need to control when the next pod should restart, you can simply delete pods one by one as necessary. A new pod will come up right after deletion (standard statefulset behavior).

For more automated solution use `kubectl rollout restart statefulset <sts-name>` on the statefulset running your Varnish pods. It will recreate the pods with the current configuration using the configured update strategy. Keep in mind that if you use `OnDelete` update strategy, it doesn't make sense to use this approach as you still have to manually delete the pods to complete the update. Use it with `RollingUpdate` and `DelayedRollingUpdate` update strategies.

### Deleting a VarnishCluster Resource

Simply calling `kubectl delete` on the `VarnishCluster` will recursively delete all dependent resources, so that is the only action you need to take. This includes a user-generated ConfigMap, as the VarnishCluster will take ownership of that ConfigMap after creation. Deleting any of the dependent resources will trigger the operator to recreate that resource, in the same way that deleting the Pod of a Deployment will trigger the recreation of that Pod.